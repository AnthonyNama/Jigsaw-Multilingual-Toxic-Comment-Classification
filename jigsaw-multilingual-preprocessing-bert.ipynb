{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data processing\n\nThis notebook preprocesses our dataset for compatibility with BERT. You should feel free to investigate other solutions (both models and tokenizers)!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, time\nimport random\nimport pandas as pd\nimport numpy as np\nimport gc\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom kaggle_datasets import KaggleDatasets\n\n# We'll use a tokenizer for the BERT model from the modelling demo notebook.\n!pip install bert-tensorflow\nimport bert.tokenization\n\nprint(tf.version.VERSION)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Set global variables\n\nSet maximum sequence length and path variables."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"SEQUENCE_LENGTH = 512\n\nDATA_PATH =  \"../input/data-jigsaw1/\"\nBERT_PATH = \"../input/bertmulti\"\nBERT_PATH_SAVEDMODEL = os.path.join(BERT_PATH, \"bert_multi_cased_L-12_H-768_A-12_2\")\n\nOUTPUT_PATH = \"/kaggle/working\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\ndf_validation = pd.read_csv(os.path.join(DATA_PATH, \"validation.csv\"))\ndf_test = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tokenizer(bert_path=BERT_PATH_SAVEDMODEL):\n    \"\"\"Obtenez le tokenizer pour une couche BERT.\"\"\"\n    bert_layer = tf.saved_model.load(bert_path)\n    bert_layer = hub.KerasLayer(bert_layer, trainable=False)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    cased = bert_layer.resolved_object.do_lower_case.numpy()\n    tf.gfile = tf.io.gfile  # bert.tokenization.load_vocab dans tokenizer\n    tokenizer = bert.tokenization.FullTokenizer(vocab_file, cased)\n  \n    return tokenizer\n\ntokenizer = get_tokenizer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nProcess individual sentences for input to BERT using the tokenizer, and then prepare the entire dataset. The same code will process the other training data files, as well as the validation and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_sentence(sentence, max_seq_length=SEQUENCE_LENGTH, tokenizer=tokenizer):\n    \"\"\"Convertit la phrase sous la forme ['input_word_ids', 'input_mask', 'segment_ids'].\"\"\"\n    # Tokenize, et tronque à max_seq_length si necessaire.\n    tokens = tokenizer.tokenize(str(sentence))\n    if len(tokens) > max_seq_length - 2:\n        tokens = tokens[:(max_seq_length - 2)]\n\n    # Convertir les tokens de la phrase en IDs\n    input_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"])\n\n    # 1 pour les vrais tokens et un 0 pour les tokens de rembourrage.\n    input_mask = [1] * len(input_ids)\n\n    # Compléter par des zéros si la séquence est inférieur à max_seq_length\n    pad_length = max_seq_length - len(input_ids)\n    input_ids.extend([0] * pad_length)\n    input_mask.extend([0] * pad_length)\n\n    # Nous avons un seul segment d'entrée\n    segment_ids = [0] * max_seq_length\n\n    return (input_ids, input_mask, segment_ids)\n\ndef preprocess_and_save_dataset(unprocessed_filename, text_label='comment_text',\n                                seq_length=SEQUENCE_LENGTH, verbose=True):\n    \"\"\"Preprocess a CSV to the expected TF Dataset form for multilingual BERT,\n    and save the result.\"\"\"\n    dataframe = pd.read_csv(os.path.join(DATA_PATH, unprocessed_filename), index_col='id')\n    processed_filename = (unprocessed_filename.rstrip('.csv') +\n                          \"-processed-seqlen{}.csv\".format(SEQUENCE_LENGTH))\n\n    pos = 0\n    start = time.time()\n\n    while pos < len(dataframe):\n        processed_df = dataframe[pos:pos + 100000].copy()\n\n        processed_df['input_word_ids'], processed_df['input_mask'], processed_df['all_segment_id'] = (\n            zip(*processed_df[text_label].apply(process_sentence)))\n        \n        processed_df.drop([\"comment_text\"], axis=1, inplace=True)\n\n        if pos == 0:\n            processed_df.to_csv(processed_filename, index_label='id', mode='w')\n        else:\n            processed_df.to_csv(processed_filename, index_label='id', mode='a', header=False)\n\n        if verbose:\n            print('Processed {} examples in {}'.format(pos + 100000, time.time() - start))\n        pos += 100000\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Process the validation dataset.\npreprocess_and_save_dataset(\"validation.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Process the test dataset.\npreprocess_and_save_dataset(\"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Process the training dataset.\npreprocess_and_save_dataset(\"train.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}