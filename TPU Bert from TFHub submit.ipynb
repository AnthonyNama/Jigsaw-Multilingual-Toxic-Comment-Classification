{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bertmulti/bert_multi_cased_L-12_H-768_A-12_2/saved_model.pb\n",
      "/kaggle/input/bertmulti/bert_multi_cased_L-12_H-768_A-12_2/assets/vocab.txt\n",
      "/kaggle/input/bertmulti/bert_multi_cased_L-12_H-768_A-12_2/variables/variables.index\n",
      "/kaggle/input/bertmulti/bert_multi_cased_L-12_H-768_A-12_2/variables/variables.data-00000-of-00001\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsawbert/train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsawbert/test-processed-seqlen128.csv\n",
      "/kaggle/input/jigsawbert/validation-processed-seqlen128.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    \n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 128\n",
    "\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path('jigsawbert')\n",
    "BERT_GCS_PATH = KaggleDatasets().get_gcs_path('bertmulti')\n",
    "BERT_GCS_PATH_SAVEDMODEL = BERT_GCS_PATH + \"/bert_multi_cased_L-12_H-768_A-12_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilingual_bert_model(max_seq_length=SEQUENCE_LENGTH, trainable_bert=True):\n",
    "    \"\"\"Construit et retourne un mod√®le Bert multilingue\"\"\"\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"all_segment_id\")\n",
    "    \n",
    "    bert_layer = tf.saved_model.load(BERT_GCS_PATH_SAVEDMODEL)\n",
    "    bert_layer = hub.KerasLayer(bert_layer, trainable=trainable_bert)\n",
    "\n",
    "    pooled_output, _ = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    output = tf.keras.layers.Dense(32, activation='relu')(pooled_output)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='labels')(output)\n",
    "\n",
    "    return tf.keras.Model(inputs={'input_word_ids': input_word_ids,'input_mask': input_mask,\n",
    "                                  'all_segment_id': segment_ids},outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string_list_into_ints(strlist):\n",
    "    s = tf.strings.strip(strlist)\n",
    "    s = tf.strings.substr(\n",
    "        strlist, 1, tf.strings.length(s) - 2)  # Remove parentheses around list\n",
    "    s = tf.strings.split(s, ',', maxsplit=SEQUENCE_LENGTH)\n",
    "    s = tf.strings.to_number(s, tf.int32)\n",
    "    s = tf.reshape(s, [SEQUENCE_LENGTH])  # Force shape here needed for XLA compilation (TPU)\n",
    "    return s\n",
    "\n",
    "def format_sentences(data, label='toxic', remove_language=False):\n",
    "    labels = {'labels': data.pop(label)}\n",
    "    if remove_language:\n",
    "        languages = {'language': data.pop('lang')}\n",
    "    # The remaining three items in the dict parsed from the CSV are lists of integers\n",
    "    for k,v in data.items():  # \"input_word_ids\", \"input_mask\", \"all_segment_id\"\n",
    "        data[k] = parse_string_list_into_ints(v)\n",
    "    return data, labels\n",
    "\n",
    "def make_sentence_dataset_from_csv(filename, label='toxic', language_to_filter=None):\n",
    "    # This assumes the column order label, input_word_ids, input_mask, segment_ids\n",
    "    SELECTED_COLUMNS = [label, \"input_word_ids\", \"input_mask\", \"all_segment_id\"]\n",
    "    label_default = tf.int32 if label == 'id' else tf.float32\n",
    "    COLUMN_DEFAULTS  = [label_default, tf.string, tf.string, tf.string]\n",
    "\n",
    "    if language_to_filter:\n",
    "        insert_pos = 0 if label != 'id' else 1\n",
    "        SELECTED_COLUMNS.insert(insert_pos, 'lang')\n",
    "        COLUMN_DEFAULTS.insert(insert_pos, tf.string)\n",
    "\n",
    "    preprocessed_sentences_dataset = tf.data.experimental.make_csv_dataset(\n",
    "        filename, column_defaults=COLUMN_DEFAULTS, select_columns=SELECTED_COLUMNS,\n",
    "        batch_size=1, num_epochs=1, shuffle=False)  # We'll do repeating and shuffling ourselves\n",
    "    # make_csv_dataset required a batch size, but we want to batch later\n",
    "    preprocessed_sentences_dataset = preprocessed_sentences_dataset.unbatch()\n",
    "    \n",
    "    if language_to_filter:\n",
    "        preprocessed_sentences_dataset = preprocessed_sentences_dataset.filter(\n",
    "            lambda data: tf.math.equal(data['lang'], tf.constant(language_to_filter)))\n",
    "        #preprocessed_sentences.pop('lang')\n",
    "    preprocessed_sentences_dataset = preprocessed_sentences_dataset.map(\n",
    "        lambda data: format_sentences(data, label=label,\n",
    "                                      remove_language=language_to_filter))\n",
    "\n",
    "    return preprocessed_sentences_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_pipeline(dataset, repeat_and_shuffle=True):\n",
    "    \"\"\"Set up the pipeline for the given dataset.\n",
    "    \n",
    "    Caches, repeats, shuffles, and sets the pipeline up to prefetch batches.\"\"\"\n",
    "    cached_dataset = dataset.cache()\n",
    "    if repeat_and_shuffle:\n",
    "        cached_dataset = cached_dataset.repeat().shuffle(2048)\n",
    "    cached_dataset = cached_dataset.batch(32 * strategy.num_replicas_in_sync)\n",
    "    cached_dataset = cached_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return cached_dataset\n",
    "\n",
    "# Load the preprocessed English dataframe.\n",
    "preprocessed_en_filename = (GCS_PATH + \"/train-processed-seqlen{}.csv\".format(SEQUENCE_LENGTH))\n",
    "\n",
    "# Set up the dataset and pipeline.\n",
    "english_train_dataset = make_dataset_pipeline(\n",
    "    make_sentence_dataset_from_csv(preprocessed_en_filename))\n",
    "\n",
    "# Process the new datasets by language.\n",
    "preprocessed_val_filename = (\n",
    "    GCS_PATH + \"/validation-processed-seqlen{}.csv\".format(SEQUENCE_LENGTH))\n",
    "\n",
    "nonenglish_val_datasets = {}\n",
    "for language_name, language_label in [('Spanish', 'es'), ('Italian', 'it'),\n",
    "                                      ('Turkish', 'tr')]:\n",
    "    nonenglish_val_datasets[language_name] = make_sentence_dataset_from_csv(\n",
    "        preprocessed_val_filename, language_to_filter=language_label)\n",
    "    nonenglish_val_datasets[language_name] = make_dataset_pipeline(\n",
    "        nonenglish_val_datasets[language_name])\n",
    "\n",
    "nonenglish_val_datasets['Combined'] = tf.data.experimental.sample_from_datasets(\n",
    "        (nonenglish_val_datasets['Spanish'], nonenglish_val_datasets['Italian'],\n",
    "         nonenglish_val_datasets['Turkish']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "all_segment_id (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 177853441   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 all_segment_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           24608       keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "labels (Dense)                  (None, 1)            33          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 177,878,082\n",
      "Trainable params: 177,878,081\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    multilingual_bert = multilingual_bert_model()\n",
    "    # Compile le mod√®le.\n",
    "    multilingual_bert.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                              metrics=[tf.keras.metrics.AUC()])\n",
    "multilingual_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 322s 107ms/step - loss: 0.1995 - auc: 0.9757 - val_loss: 0.4042 - val_auc: 0.7760\n",
      "Epoch 2/2\n",
      "3000/3000 [==============================] - 314s 105ms/step - loss: 0.1421 - auc: 0.9873 - val_loss: 0.4054 - val_auc: 0.7868\n"
     ]
    }
   ],
   "source": [
    "# Train on English Wikipedia comment data.\n",
    "history = multilingual_bert.fit(english_train_dataset, steps_per_epoch=3000, epochs=2, verbose=1,\n",
    "                                validation_data=nonenglish_val_datasets['Combined'], validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dataset...\n",
      "Computing predictions...\n",
      "[0.08343673 0.12814897 0.25638998 ... 0.66844535 0.27262563 0.04352087]\n",
      "Generating submission file...\n",
      "id,toxic\r\n",
      "0,0.083437\r\n",
      "1,0.128149\r\n",
      "2,0.256390\r\n",
      "3,0.082323\r\n",
      "4,0.005595\r\n",
      "5,0.358424\r\n",
      "6,0.003955\r\n",
      "7,0.146809\r\n",
      "8,0.433439\r\n"
     ]
    }
   ],
   "source": [
    "TEST_DATASET_SIZE = 63812\n",
    "\n",
    "print('Making dataset...')\n",
    "preprocessed_test_filename = (\n",
    "    GCS_PATH + \"/test-processed-seqlen{}.csv\".format(SEQUENCE_LENGTH))\n",
    "test_dataset = make_sentence_dataset_from_csv(preprocessed_test_filename, label='id')\n",
    "test_dataset = make_dataset_pipeline(test_dataset, repeat_and_shuffle=False)\n",
    "\n",
    "print('Computing predictions...')\n",
    "test_sentences_dataset = test_dataset.map(lambda sentence, idnum: sentence)\n",
    "probabilities = np.squeeze(multilingual_bert.predict(test_sentences_dataset))\n",
    "print(probabilities)\n",
    "\n",
    "print('Generating submission file...')\n",
    "test_ids_dataset = test_dataset.map(lambda sentence, idnum: idnum).unbatch()\n",
    "test_ids = next(iter(test_ids_dataset.batch(TEST_DATASET_SIZE)))[\n",
    "    'labels'].numpy().astype('U')  # All in one batch\n",
    "\n",
    "np.savetxt('submission.csv', np.rec.fromarrays([test_ids, probabilities]),\n",
    "           fmt=['%s', '%f'], delimiter=',', header='id,toxic', comments='')\n",
    "!head submission.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
